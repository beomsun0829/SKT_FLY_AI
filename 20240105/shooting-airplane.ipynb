{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsIXEH5vPeUB",
        "outputId": "d5054e9c-c487-4f9b-f643-80f775f74042"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "env = gym.make('gym_examples:gym_examples/ShootingAirplane-v0', render_mode='text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYCQ5y0fP0mO",
        "outputId": "96ab53b7-7a17-4f6b-b4e4-6ad25958c6fd"
      },
      "outputs": [],
      "source": [
        "obs, info = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utV59F4hmywr",
        "outputId": "6680b539-a6bb-482c-e2b7-5903b59636a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Box(0, 255, (8, 8, 1), uint8)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ2E5wU4QqAj",
        "outputId": "e9b95fa3-29ac-412f-d7a2-91213bf1dd79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 8, 1)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "obs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLJMo4TSQrIy",
        "outputId": "59504dc3-369f-4c15-e4ef-14bc8ec8333c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |         \n",
            "         |    HHH  \n",
            "         |     H   \n",
            "         |   HHHHH \n",
            "         |     H   \n",
            "         |         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LwsoJLX0nTMC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "obs, reward, done, _, info = env.step((0, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZegK_bTnhr0",
        "outputId": "154861ed-11dd-4ff5-9811-b2a6f61fb1a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M        |         \n",
            "         |         \n",
            "         |         \n",
            "         |    HHH  \n",
            "         |     H   \n",
            "         |   HHHHH \n",
            "         |     H   \n",
            "         |         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnSgMbtHnppw",
        "outputId": "18df42e6-75e6-4321-be82-4c706e49fac1"
      },
      "outputs": [],
      "source": [
        "obs, reward, done, _, info = env.step((4, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah-LJysvn0sq",
        "outputId": "9dab913e-257c-4976-8214-9988bf683bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M        |         \n",
            "         |         \n",
            "         |         \n",
            "         |    HHH  \n",
            "    H    |     H   \n",
            "         |   HHHHH \n",
            "         |     H   \n",
            "         |         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q2T3wb5n4tL",
        "outputId": "3607d12c-9909-483f-f42f-fc7af7d422a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M        |         \n",
            "         |         \n",
            "         |         \n",
            "   H     |    HHH  \n",
            "    H    |     H   \n",
            "         |   HHHHH \n",
            "         |     H   \n",
            "         |         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "obs, reward, done, _, info = env.step((3, 3))\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARJXRh1zn-G0",
        "outputId": "d5d48d91-adb4-4932-cffb-3fee96d3ea9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj0RBIQCoAPB",
        "outputId": "012bb7e0-9734-494e-b136-b897d41356f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M        |         \n",
            "   M     |         \n",
            "         |         \n",
            "   H     |    HHH  \n",
            "    H    |     H   \n",
            "         |   HHHHH \n",
            "         |     H   \n",
            "         |         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "obs, reward, done, _, info = env.step((1, 3))\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrJRnYG6oDpf",
        "outputId": "4db1bd9d-880e-406f-a391-3e28ae3a6595"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK9qxD9t288H"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSdYfDKwRMOJ",
        "outputId": "227ff1ad-1b85-4f82-f8a5-84c7989f8be1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v7gRdlQ3ITb",
        "outputId": "7abd6431-3b84-44ca-ef0b-a2878f3c3060"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_max = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_interval = epsilon_max - epsilon_min\n",
        "batch_size = 16\n",
        "max_steps_per_episode = 60\n",
        "max_episodes = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD_N7ySI3iXE",
        "outputId": "06e860ab-b721-4b9d-8edf-730ddeefb4a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Box(0, 255, (8, 8, 1), uint8)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAs46CeE3p73",
        "outputId": "51b91ac1-332a-47fd-a135-36d4c0f03fc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiDiscrete([8 8])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3Shy3sL3toi",
        "outputId": "ba573a3d-137a-46cb-c5a7-1a0c17ae6e9f"
      },
      "outputs": [],
      "source": [
        "num_actions = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrxC01ff3zHN",
        "outputId": "3967a819-1fb3-48df-c5d0-2e0ee3b905e6"
      },
      "outputs": [],
      "source": [
        "class QModel(nn.Module):\n",
        "  def __init__(self, num_actions):\n",
        "    super(QModel, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n",
        "    self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(1152, 512)\n",
        "    self.fc2 = nn.Linear(512, num_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = nn.functional.relu(self.conv1(x))\n",
        "    x = nn.functional.relu(self.conv2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = nn.functional.relu(self.conv3(x))\n",
        "    x = self.flatten(x)\n",
        "    x = nn.functional.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    return self.fc2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IqNrPCX5Ua6",
        "outputId": "8f976677-b23b-4732-e75d-a07cbb756172"
      },
      "outputs": [],
      "source": [
        "model = QModel(num_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r25E_DsH5dpJ",
        "outputId": "0a7cd057-a6fc-4614-c2b5-04ba5fbe2d2a"
      },
      "outputs": [],
      "source": [
        "model_target = QModel(num_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vgSKetaK5gYl"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH941GPa5xiQ",
        "outputId": "2387feef-50a1-4b45-9bff-f40967e9fc0e"
      },
      "outputs": [],
      "source": [
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXZhFxRY5_VP",
        "outputId": "b3c57b64-8653-4d34-d2f9-5bba29035a90"
      },
      "outputs": [],
      "source": [
        "episode_reward_history = []\n",
        "running_reward = 0.\n",
        "episode_count = 0\n",
        "frame_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "i1qSeSvE6JDy"
      },
      "outputs": [],
      "source": [
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 200000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 500000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opkmq0qY6bS2",
        "outputId": "683cf263-49ce-4cee-af90-3a9fee675f99"
      },
      "outputs": [],
      "source": [
        "def preprocess_state(obs):\n",
        "  st = torch.from_numpy(obs).squeeze()\n",
        "  st = st.to(torch.int64)\n",
        "  st = torch.nn.functional.one_hot(st, num_classes=3)\n",
        "  st = st.permute(2, 0, 1)\n",
        "  return st.to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u3tsDnh68Fw",
        "outputId": "fdf2df01-ce26-4999-fc95-53f76e2dab62"
      },
      "outputs": [],
      "source": [
        "board, info = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R3edFrV7A9j",
        "outputId": "8697b950-752b-45b9-b81a-685187c1c947"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 8, 1)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "board.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4ASGws87CA0",
        "outputId": "4459f076-7004-403c-a673-09732636c7d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 8, 8])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st = preprocess_state(board)\n",
        "st.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNmJgW_V7HNn",
        "outputId": "8b01a290-aa6b-4727-dd1d-b176ab457fde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gbMTCOca7L6J"
      },
      "outputs": [],
      "source": [
        "def get_greedy_epsilon(model, state, mask):\n",
        "  global epsilon\n",
        "\n",
        "  #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n",
        "  if np.random.rand(1)[0] < epsilon:\n",
        "    action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      # add a batch axis\n",
        "      state_tensor = state.unsqueeze(0)\n",
        "      # compute the q-values\n",
        "      q_values = model(state_tensor)\n",
        "      # select the q-values of valid actions\n",
        "      action = torch.argmax(\n",
        "        q_values.squeeze() + torch.from_numpy(mask) * 100., dim=0)\n",
        "\n",
        "  epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "  epsilon = max(epsilon, epsilon_min)\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-M96FnX77z_B"
      },
      "outputs": [],
      "source": [
        "def get_greedy_action(model, state, mask):\n",
        "  global epsilon\n",
        "  with torch.no_grad():\n",
        "    state_tensor = state.unsqueeze(0) # batch dimension\n",
        "    q_values = model(state_tensor)\n",
        "    action = torch.argmax(\n",
        "      q_values.squeeze() + torch.from_numpy(mask) * 100.,dim=0) \n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5Ddv42HL7_qM"
      },
      "outputs": [],
      "source": [
        "def sample_batch(_batch_size):\n",
        "  indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n",
        "  state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n",
        "  state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n",
        "  rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
        "  action_sample = np.array([action_history[i] for i in indices])\n",
        "\n",
        "  # action mask is the mask for the valid actions at the '''next''' state\n",
        "  action_mask_sample = np.array([action_mask_history[i] for i in indices])\n",
        "  done_sample = np.array([float(done_history[i]) for i in indices])\n",
        "  return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWDqDT-j8xo_",
        "outputId": "61fb9f19-92fd-4d16-c055-ffd14efe8878"
      },
      "outputs": [],
      "source": [
        "def update_network():\n",
        "  state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n",
        "    sample_batch(batch_size)\n",
        "\n",
        "  state_sample = torch.tensor(state_sample, dtype=torch.float32)\n",
        "  state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32)\n",
        "  action_sample = torch.tensor(action_sample, dtype=torch.int64)\n",
        "  action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64)\n",
        "  rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32)\n",
        "  done_sample = torch.tensor(done_sample, dtype=torch.float32)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    future_rewards = model_target(state_next_sample)            # off policy, target and behavior policy are different, update later\n",
        "    max_q_values = torch.max (\n",
        "        future_rewards + action_mask_sample * 100., dim=1).values.detach() - 100.\n",
        "    target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n",
        "\n",
        "  q_values = model(state_sample)\n",
        "  q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n",
        "  loss = loss_function(q_values_action, target_q_values)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ji7__Lql-IWh",
        "outputId": "6fda3e4f-fb01-4cea-c9b3-562642c25711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10 reward: -36 running reward: -36.9 epsilon: 0.9974935000000207\n",
            "Episode 20 reward: -39 running reward: -37.9 epsilon: 0.9949240000000419\n",
            "Episode 30 reward: -41 running reward: -38.266666666666666 epsilon: 0.992359000000063\n",
            "Episode 40 reward: -34 running reward: -38.075 epsilon: 0.9898525000000837\n",
            "Episode 50 reward: -35 running reward: -38.4 epsilon: 0.9872470000001052\n",
            "Episode 60 reward: -20 running reward: -37.78333333333333 epsilon: 0.9848305000001252\n",
            "Episode 70 reward: -41 running reward: -37.857142857142854 epsilon: 0.9822520000001465\n",
            "Episode 80 reward: -38 running reward: -37.9625 epsilon: 0.9796915000001676\n",
            "Episode 90 reward: -29 running reward: -37.94444444444444 epsilon: 0.9771445000001886\n",
            "Episode 100 reward: -39 running reward: -38.13 epsilon: 0.9745255000002102\n",
            "Episode 110 reward: -33 running reward: -38.17 epsilon: 0.9719830000002312\n",
            "Episode 120 reward: -43 running reward: -37.7 epsilon: 0.9695710000002511\n",
            "Episode 130 reward: -38 running reward: -37.4 epsilon: 0.9670690000002717\n",
            "Episode 140 reward: -41 running reward: -36.85 epsilon: 0.9647650000002908\n",
            "Episode 150 reward: -41 running reward: -36.86 epsilon: 0.9621640000003122\n",
            "Episode 160 reward: -41 running reward: -37.07 epsilon: 0.9596710000003328\n",
            "Episode 170 reward: -43 running reward: -37.11 epsilon: 0.957101500000354\n",
            "Episode 180 reward: -41 running reward: -37.01 epsilon: 0.954559000000375\n",
            "Episode 190 reward: -39 running reward: -36.95 epsilon: 0.9520480000003957\n",
            "Episode 200 reward: -31 running reward: -36.5 epsilon: 0.9495955000004159\n",
            "Episode 210 reward: -39 running reward: -36.18 epsilon: 0.9471880000004358\n",
            "Episode 220 reward: -37 running reward: -36.55 epsilon: 0.9446455000004568\n",
            "Episode 230 reward: -41 running reward: -36.82 epsilon: 0.942067000000478\n",
            "Episode 240 reward: -19 running reward: -36.92 epsilon: 0.9397180000004974\n",
            "Episode 250 reward: -38 running reward: -36.78 epsilon: 0.9371710000005185\n",
            "Episode 260 reward: -26 running reward: -36.17 epsilon: 0.9349075000005371\n",
            "Episode 270 reward: -39 running reward: -35.9 epsilon: 0.9324145000005577\n",
            "Episode 280 reward: -25 running reward: -35.66 epsilon: 0.929953000000578\n",
            "Episode 290 reward: -29 running reward: -35.28 epsilon: 0.9275590000005978\n",
            "Episode 300 reward: -41 running reward: -35.25 epsilon: 0.9251200000006179\n",
            "Episode 310 reward: -32 running reward: -35.12 epsilon: 0.9227710000006373\n",
            "Episode 320 reward: -43 running reward: -35.08 epsilon: 0.9202465000006581\n",
            "Episode 330 reward: -34 running reward: -34.56 epsilon: 0.9178660000006778\n",
            "Episode 340 reward: -37 running reward: -34.46 epsilon: 0.9155260000006971\n",
            "Episode 350 reward: -31 running reward: -33.85 epsilon: 0.9131815000007164\n",
            "Episode 360 reward: -31 running reward: -34.12 epsilon: 0.910805500000736\n",
            "Episode 370 reward: -41 running reward: -33.48 epsilon: 0.9085915000007543\n",
            "Episode 380 reward: -41 running reward: -33.24 epsilon: 0.9062290000007738\n",
            "Episode 390 reward: -32 running reward: -33.3 epsilon: 0.9038260000007936\n",
            "Episode 400 reward: -41 running reward: -32.53 epsilon: 0.901715500000811\n",
            "Episode 410 reward: -27 running reward: -32.53 epsilon: 0.8993665000008304\n",
            "Episode 420 reward: -30 running reward: -32.06 epsilon: 0.8970085000008499\n",
            "Episode 430 reward: -38 running reward: -31.94 epsilon: 0.894691000000869\n",
            "Episode 440 reward: -38 running reward: -32.13 epsilon: 0.8922835000008889\n",
            "Episode 450 reward: -11 running reward: -31.68 epsilon: 0.8901505000009065\n",
            "Episode 460 reward: -24 running reward: -31.45 epsilon: 0.8878690000009253\n",
            "Episode 470 reward: -25 running reward: -31.35 epsilon: 0.8856910000009433\n",
            "Episode 480 reward: -41 running reward: -31.4 epsilon: 0.8833150000009629\n",
            "Episode 490 reward: -41 running reward: -31.62 epsilon: 0.8808220000009834\n",
            "Episode 500 reward: -18 running reward: -31.65 epsilon: 0.8786800000010011\n",
            "Episode 510 reward: -32 running reward: -31.35 epsilon: 0.8764570000010194\n",
            "Episode 520 reward: -33 running reward: -30.8 epsilon: 0.8743375000010369\n",
            "Episode 530 reward: -38 running reward: -30.17 epsilon: 0.872267500001054\n",
            "Episode 540 reward: -18 running reward: -29.71 epsilon: 0.8700670000010722\n",
            "Episode 550 reward: -35 running reward: -29.52 epsilon: 0.8680105000010891\n",
            "Episode 560 reward: -13 running reward: -29.08 epsilon: 0.8659270000011063\n",
            "Episode 570 reward: -29 running reward: -29.19 epsilon: 0.8636995000011247\n",
            "Episode 580 reward: -41 running reward: -29.13 epsilon: 0.8613415000011442\n",
            "Episode 590 reward: -27 running reward: -28.32 epsilon: 0.859186000001162\n",
            "Episode 600 reward: -25 running reward: -28.3 epsilon: 0.8570620000011795\n",
            "Episode 610 reward: -14 running reward: -27.91 epsilon: 0.8550055000011965\n",
            "Episode 620 reward: -20 running reward: -27.71 epsilon: 0.8529670000012133\n",
            "Episode 630 reward: -16 running reward: -28.4 epsilon: 0.8506135000012327\n",
            "Episode 640 reward: -16 running reward: -28.6 epsilon: 0.8483320000012515\n",
            "Episode 650 reward: -23 running reward: -28.61 epsilon: 0.8462710000012685\n",
            "Episode 660 reward: -33 running reward: -29.05 epsilon: 0.8439715000012875\n",
            "Episode 670 reward: -20 running reward: -29.23 epsilon: 0.8416720000013065\n",
            "Episode 680 reward: -30 running reward: -28.44 epsilon: 0.839669500001323\n",
            "Episode 690 reward: -38 running reward: -28.06 epsilon: 0.8376760000013395\n",
            "Episode 700 reward: -29 running reward: -28.1 epsilon: 0.8355340000013571\n",
            "Episode 710 reward: -22 running reward: -28.37 epsilon: 0.8333560000013751\n",
            "Episode 720 reward: -30 running reward: -28.32 epsilon: 0.8313400000013917\n",
            "Episode 730 reward: -32 running reward: -28.05 epsilon: 0.8290810000014104\n",
            "Episode 740 reward: -35 running reward: -27.82 epsilon: 0.8268760000014286\n",
            "Episode 750 reward: -41 running reward: -27.96 epsilon: 0.8247520000014461\n",
            "Episode 760 reward: -29 running reward: -27.72 epsilon: 0.8225695000014641\n",
            "Episode 770 reward: -14 running reward: -26.82 epsilon: 0.8206570000014799\n",
            "Episode 780 reward: -20 running reward: -27.18 epsilon: 0.8184835000014978\n",
            "Episode 790 reward: -27 running reward: -27.4 epsilon: 0.816409000001515\n",
            "Episode 800 reward: -27 running reward: -27.53 epsilon: 0.814226500001533\n",
            "Episode 810 reward: -37 running reward: -27.36 epsilon: 0.8121160000015504\n",
            "Episode 820 reward: -24 running reward: -27.68 epsilon: 0.809974000001568\n",
            "Episode 830 reward: -22 running reward: -27.1 epsilon: 0.8079940000015844\n",
            "Episode 840 reward: -41 running reward: -26.76 epsilon: 0.8059510000016012\n",
            "Episode 850 reward: -17 running reward: -26.24 epsilon: 0.8040610000016168\n",
            "Episode 860 reward: -39 running reward: -25.95 epsilon: 0.8020000000016339\n",
            "Episode 870 reward: -23 running reward: -25.86 epsilon: 0.8001280000016493\n",
            "Episode 880 reward: -16 running reward: -24.92 epsilon: 0.7983685000016638\n",
            "Episode 890 reward: -14 running reward: -24.55 epsilon: 0.7964425000016797\n",
            "Episode 900 reward: -12 running reward: -23.82 epsilon: 0.7945615000016952\n",
            "Episode 910 reward: -37 running reward: -23.32 epsilon: 0.7926850000017107\n",
            "Episode 920 reward: -25 running reward: -23.23 epsilon: 0.790592500001728\n",
            "Episode 930 reward: -1 running reward: -23.04 epsilon: 0.7886800000017438\n",
            "Episode 940 reward: -28 running reward: -22.86 epsilon: 0.78670900000176\n",
            "Episode 950 reward: -19 running reward: -22.89 epsilon: 0.7848055000017757\n",
            "Episode 960 reward: -20 running reward: -22.51 epsilon: 0.7829155000017913\n",
            "Episode 970 reward: -14 running reward: -22.66 epsilon: 0.7809760000018073\n",
            "Episode 980 reward: -15 running reward: -22.46 epsilon: 0.7793065000018211\n",
            "Episode 990 reward: -16 running reward: -22.39 epsilon: 0.7774120000018367\n",
            "Episode 1000 reward: -37 running reward: -22.36 epsilon: 0.7755535000018521\n",
            "Episode 1010 reward: -20 running reward: -22.51 epsilon: 0.7736095000018681\n",
            "Episode 1020 reward: -24 running reward: -21.9 epsilon: 0.7717645000018833\n",
            "Episode 1030 reward: -18 running reward: -21.52 epsilon: 0.7700230000018977\n",
            "Episode 1040 reward: -26 running reward: -21.67 epsilon: 0.7679935000019145\n",
            "Episode 1050 reward: -27 running reward: -22.11 epsilon: 0.7658830000019319\n",
            "Episode 1060 reward: -21 running reward: -22.28 epsilon: 0.7639165000019481\n",
            "Episode 1070 reward: -26 running reward: -22.37 epsilon: 0.7619365000019644\n",
            "Episode 1080 reward: -29 running reward: -22.95 epsilon: 0.7600150000019803\n",
            "Episode 1090 reward: -14 running reward: -22.96 epsilon: 0.758116000001996\n",
            "Episode 1100 reward: -15 running reward: -22.69 epsilon: 0.7563790000020103\n",
            "Episode 1110 reward: -17 running reward: -22.37 epsilon: 0.7545700000020252\n",
            "Episode 1120 reward: -22 running reward: -21.84 epsilon: 0.7529635000020385\n",
            "Episode 1130 reward: -3 running reward: -21.45 epsilon: 0.7513975000020514\n",
            "Episode 1140 reward: -15 running reward: -21.09 epsilon: 0.7495300000020668\n",
            "Episode 1150 reward: -21 running reward: -20.72 epsilon: 0.7475860000020829\n",
            "Episode 1160 reward: -15 running reward: -20.11 epsilon: 0.7458940000020968\n",
            "Episode 1170 reward: -9 running reward: -19.99 epsilon: 0.7439680000021127\n",
            "Episode 1180 reward: -25 running reward: -19.57 epsilon: 0.7422265000021271\n",
            "Episode 1190 reward: -30 running reward: -19.41 epsilon: 0.7404085000021421\n",
            "Episode 1200 reward: -31 running reward: -18.93 epsilon: 0.7388785000021547\n",
            "Episode 1210 reward: -18 running reward: -18.84 epsilon: 0.7371100000021693\n",
            "Episode 1220 reward: -26 running reward: -19.2 epsilon: 0.7353415000021839\n",
            "Episode 1230 reward: -19 running reward: -19.78 epsilon: 0.7335325000021988\n",
            "Episode 1240 reward: -15 running reward: -19.29 epsilon: 0.7318765000022125\n",
            "Episode 1250 reward: -14 running reward: -19.15 epsilon: 0.729995500002228\n",
            "Episode 1260 reward: -19 running reward: -19.1 epsilon: 0.7283260000022418\n",
            "Episode 1270 reward: -10 running reward: -18.48 epsilon: 0.7266790000022554\n",
            "Episode 1280 reward: -22 running reward: -18.45 epsilon: 0.7249510000022696\n",
            "Episode 1290 reward: -1 running reward: -17.74 epsilon: 0.7234435000022821\n",
            "Episode 1300 reward: -16 running reward: -18.21 epsilon: 0.7217020000022965\n",
            "Episode 1310 reward: -11 running reward: -17.86 epsilon: 0.7200910000023097\n",
            "Episode 1320 reward: -21 running reward: -18.02 epsilon: 0.7182505000023249\n",
            "Episode 1330 reward: -20 running reward: -18.05 epsilon: 0.7164100000023401\n",
            "Episode 1340 reward: -25 running reward: -18.3 epsilon: 0.7146415000023547\n",
            "Episode 1350 reward: -19 running reward: -18.04 epsilon: 0.7128775000023693\n",
            "Episode 1360 reward: -19 running reward: -18.14 epsilon: 0.7111630000023834\n",
            "Episode 1370 reward: -30 running reward: -18.43 epsilon: 0.7093855000023981\n",
            "Episode 1380 reward: -35 running reward: -18.74 epsilon: 0.7075180000024135\n",
            "Episode 1390 reward: -18 running reward: -19.0 epsilon: 0.7058935000024269\n",
            "Episode 1400 reward: -25 running reward: -19.02 epsilon: 0.7041430000024413\n",
            "Episode 1410 reward: -27 running reward: -19.65 epsilon: 0.702248500002457\n",
            "Episode 1420 reward: -30 running reward: -19.86 epsilon: 0.7003135000024729\n",
            "Episode 1430 reward: -8 running reward: -18.95 epsilon: 0.6988825000024848\n",
            "Episode 1440 reward: -9 running reward: -18.64 epsilon: 0.6972535000024982\n",
            "Episode 1450 reward: -27 running reward: -18.4 epsilon: 0.6955975000025119\n",
            "Episode 1460 reward: -13 running reward: -18.54 epsilon: 0.6938290000025265\n",
            "Episode 1470 reward: -17 running reward: -18.42 epsilon: 0.6921055000025407\n",
            "Episode 1480 reward: -20 running reward: -18.36 epsilon: 0.6902650000025559\n",
            "Episode 1490 reward: -25 running reward: -18.64 epsilon: 0.6885235000025702\n",
            "Episode 1500 reward: -6 running reward: -18.36 epsilon: 0.6868990000025836\n",
            "Episode 1510 reward: -11 running reward: -17.48 epsilon: 0.685400500002596\n",
            "Episode 1520 reward: -19 running reward: -17.03 epsilon: 0.6836770000026102\n",
            "Episode 1530 reward: -17 running reward: -17.62 epsilon: 0.6819805000026242\n",
            "Episode 1540 reward: -9 running reward: -17.63 epsilon: 0.6803560000026376\n",
            "Episode 1550 reward: -13 running reward: -17.9 epsilon: 0.6785785000026523\n",
            "Episode 1560 reward: -23 running reward: -17.7 epsilon: 0.6768910000026662\n",
            "Episode 1570 reward: -14 running reward: -17.47 epsilon: 0.6752710000026796\n",
            "Episode 1580 reward: -11 running reward: -16.52 epsilon: 0.6738580000026912\n",
            "Episode 1590 reward: -27 running reward: -15.83 epsilon: 0.6724180000027031\n",
            "Episode 1600 reward: -17 running reward: -15.85 epsilon: 0.6707845000027166\n",
            "Episode 1610 reward: -24 running reward: -15.93 epsilon: 0.6692500000027293\n",
            "Episode 1620 reward: -21 running reward: -15.42 epsilon: 0.6677470000027417\n",
            "Episode 1630 reward: -18 running reward: -15.11 epsilon: 0.6661900000027545\n",
            "Episode 1640 reward: -14 running reward: -15.12 epsilon: 0.664552000002768\n",
            "Episode 1650 reward: -28 running reward: -14.73 epsilon: 0.6629500000027813\n",
            "Episode 1660 reward: -5 running reward: -14.35 epsilon: 0.6614335000027938\n",
            "Episode 1670 reward: -16 running reward: -14.29 epsilon: 0.6598495000028068\n",
            "Episode 1680 reward: -12 running reward: -14.75 epsilon: 0.6582295000028202\n",
            "Episode 1690 reward: -9 running reward: -14.98 epsilon: 0.656686000002833\n",
            "Episode 1700 reward: -20 running reward: -14.97 epsilon: 0.6550570000028464\n",
            "Episode 1710 reward: -27 running reward: -14.83 epsilon: 0.6535855000028585\n",
            "Episode 1720 reward: -22 running reward: -15.15 epsilon: 0.6519385000028721\n",
            "Episode 1730 reward: -25 running reward: -14.93 epsilon: 0.6504805000028842\n",
            "Episode 1740 reward: -17 running reward: -15.04 epsilon: 0.6487930000028981\n",
            "Episode 1750 reward: -24 running reward: -14.71 epsilon: 0.6473395000029101\n",
            "Episode 1760 reward: -26 running reward: -15.14 epsilon: 0.6456295000029242\n",
            "Episode 1770 reward: -24 running reward: -15.33 epsilon: 0.643951000002938\n",
            "Episode 1780 reward: -29 running reward: -15.05 epsilon: 0.6424570000029504\n",
            "Episode 1790 reward: -11 running reward: -15.05 epsilon: 0.6409135000029631\n",
            "Episode 1800 reward: -27 running reward: -14.78 epsilon: 0.6394060000029755\n",
            "Episode 1810 reward: -12 running reward: -15.02 epsilon: 0.6378265000029886\n",
            "Episode 1820 reward: -22 running reward: -14.52 epsilon: 0.6364045000030003\n",
            "Episode 1830 reward: -14 running reward: -14.65 epsilon: 0.6348880000030128\n",
            "Episode 1840 reward: -6 running reward: -14.11 epsilon: 0.6334435000030247\n",
            "Episode 1850 reward: -18 running reward: -14.29 epsilon: 0.6319090000030374\n",
            "Episode 1860 reward: -11 running reward: -13.99 epsilon: 0.6303340000030504\n",
            "Episode 1870 reward: -9 running reward: -13.61 epsilon: 0.6288265000030628\n",
            "Episode 1880 reward: -11 running reward: -13.25 epsilon: 0.6274945000030738\n",
            "Episode 1890 reward: -18 running reward: -13.28 epsilon: 0.6259375000030867\n",
            "Episode 1900 reward: -8 running reward: -12.92 epsilon: 0.6245920000030978\n",
            "Episode 1910 reward: -15 running reward: -12.84 epsilon: 0.6230485000031105\n",
            "Episode 1920 reward: -4 running reward: -13.12 epsilon: 0.6215005000031233\n",
            "Episode 1930 reward: -10 running reward: -13.14 epsilon: 0.6199750000031359\n",
            "Episode 1940 reward: -6 running reward: -13.21 epsilon: 0.6184990000031481\n",
            "Episode 1950 reward: -14 running reward: -12.92 epsilon: 0.6170950000031596\n",
            "Episode 1960 reward: -18 running reward: -13.12 epsilon: 0.6154300000031734\n",
            "Episode 1970 reward: -22 running reward: -13.14 epsilon: 0.6139135000031859\n",
            "Episode 1980 reward: -21 running reward: -13.23 epsilon: 0.6125410000031972\n",
            "Episode 1990 reward: -17 running reward: -13.07 epsilon: 0.6110560000032095\n",
            "Episode 2000 reward: -14 running reward: -13.57 epsilon: 0.6094855000032224\n",
            "Episode 2010 reward: -15 running reward: -13.23 epsilon: 0.6080950000032339\n",
            "Episode 2020 reward: -32 running reward: -13.0 epsilon: 0.6066505000032458\n",
            "Episode 2030 reward: -15 running reward: -13.27 epsilon: 0.6050035000032594\n",
            "Episode 2040 reward: -16 running reward: -13.31 epsilon: 0.6035095000032717\n",
            "Episode 2050 reward: -17 running reward: -14.0 epsilon: 0.6017950000032859\n",
            "Episode 2060 reward: -12 running reward: -13.16 epsilon: 0.6005080000032965\n",
            "Episode 2070 reward: -9 running reward: -13.05 epsilon: 0.5990410000033086\n",
            "Episode 2080 reward: -9 running reward: -13.0 epsilon: 0.5976910000033198\n",
            "Episode 2090 reward: -10 running reward: -12.66 epsilon: 0.5963590000033308\n",
            "Episode 2100 reward: -23 running reward: -12.61 epsilon: 0.5948110000033435\n",
            "Episode 2110 reward: -27 running reward: -12.78 epsilon: 0.5933440000033556\n",
            "Episode 2120 reward: -1 running reward: -12.66 epsilon: 0.5919535000033671\n",
            "Episode 2130 reward: -16 running reward: -12.19 epsilon: 0.590518000003379\n",
            "Episode 2140 reward: -4 running reward: -11.89 epsilon: 0.5891590000033902\n",
            "Episode 2150 reward: -14 running reward: -10.81 epsilon: 0.5879305000034003\n",
            "Episode 2160 reward: 2 running reward: -10.83 epsilon: 0.586634500003411\n",
            "Episode 2170 reward: -10 running reward: -10.51 epsilon: 0.5853115000034219\n",
            "Episode 2180 reward: -8 running reward: -10.65 epsilon: 0.5838985000034336\n",
            "Episode 2190 reward: -5 running reward: -10.93 epsilon: 0.5824405000034456\n",
            "Episode 2200 reward: -18 running reward: -10.56 epsilon: 0.581059000003457\n",
            "Episode 2210 reward: -16 running reward: -10.45 epsilon: 0.5796415000034687\n",
            "Episode 2220 reward: -6 running reward: -10.3 epsilon: 0.5783185000034796\n",
            "Episode 2230 reward: -13 running reward: -10.05 epsilon: 0.5769955000034905\n",
            "Episode 2240 reward: -10 running reward: -10.01 epsilon: 0.5756545000035016\n",
            "Episode 2250 reward: -10 running reward: -10.23 epsilon: 0.5743270000035126\n",
            "Episode 2260 reward: -2 running reward: -10.15 epsilon: 0.573067000003523\n",
            "Episode 2270 reward: -25 running reward: -10.32 epsilon: 0.5716675000035345\n",
            "Episode 2280 reward: -1 running reward: -10.29 epsilon: 0.570268000003546\n",
            "Episode 2290 reward: -17 running reward: -10.03 epsilon: 0.5689270000035571\n",
            "Episode 2300 reward: -16 running reward: -10.04 epsilon: 0.5675410000035686\n",
            "Episode 2310 reward: -2 running reward: -10.07 epsilon: 0.5661100000035804\n",
            "Episode 2320 reward: -6 running reward: -10.12 epsilon: 0.5647645000035915\n",
            "Episode 2330 reward: -10 running reward: -10.3 epsilon: 0.563360500003603\n",
            "Episode 2340 reward: -12 running reward: -10.22 epsilon: 0.5620555000036138\n",
            "Episode 2350 reward: -12 running reward: -10.71 epsilon: 0.5605075000036266\n",
            "Episode 2360 reward: -10 running reward: -10.99 epsilon: 0.559121500003638\n",
            "Episode 2370 reward: -19 running reward: -10.64 epsilon: 0.5578795000036483\n",
            "Episode 2380 reward: -1 running reward: -10.09 epsilon: 0.5567275000036578\n",
            "Episode 2390 reward: -11 running reward: -10.35 epsilon: 0.5552695000036698\n",
            "Episode 2400 reward: -5 running reward: -10.01 epsilon: 0.55403650000368\n",
            "Episode 2410 reward: -10 running reward: -9.53 epsilon: 0.55282150000369\n",
            "Episode 2420 reward: -13 running reward: -9.58 epsilon: 0.5514535000037013\n",
            "Episode 2430 reward: -13 running reward: -9.38 epsilon: 0.5501395000037121\n",
            "Episode 2440 reward: -9 running reward: -9.7 epsilon: 0.5486905000037241\n",
            "Episode 2450 reward: -1 running reward: -8.94 epsilon: 0.547484500003734\n",
            "Episode 2460 reward: -18 running reward: -8.86 epsilon: 0.5461345000037452\n",
            "Episode 2470 reward: -7 running reward: -8.72 epsilon: 0.5449555000037549\n",
            "Episode 2480 reward: -2 running reward: -9.09 epsilon: 0.5436370000037658\n",
            "Episode 2490 reward: -20 running reward: -8.95 epsilon: 0.5422420000037773\n",
            "Episode 2500 reward: -17 running reward: -8.86 epsilon: 0.5410495000037872\n",
            "Episode 2510 reward: -14 running reward: -9.09 epsilon: 0.539731000003798\n",
            "Episode 2520 reward: -4 running reward: -8.99 epsilon: 0.538408000003809\n",
            "Episode 2530 reward: -21 running reward: -9.09 epsilon: 0.5370490000038202\n",
            "Episode 2540 reward: -11 running reward: -8.71 epsilon: 0.5357710000038307\n",
            "Episode 2550 reward: -5 running reward: -8.79 epsilon: 0.534529000003841\n",
            "Episode 2560 reward: -2 running reward: -8.69 epsilon: 0.5332240000038517\n",
            "Episode 2570 reward: -15 running reward: -8.9 epsilon: 0.5319505000038622\n",
            "Episode 2580 reward: 1 running reward: -8.53 epsilon: 0.5307985000038717\n",
            "Episode 2590 reward: -15 running reward: -8.03 epsilon: 0.5296285000038814\n",
            "Episode 2600 reward: -5 running reward: -8.52 epsilon: 0.5282155000038931\n",
            "Episode 2610 reward: -4 running reward: -8.54 epsilon: 0.526888000003904\n",
            "Episode 2620 reward: 1 running reward: -8.28 epsilon: 0.525682000003914\n",
            "Episode 2630 reward: -6 running reward: -8.42 epsilon: 0.5242600000039257\n",
            "Episode 2640 reward: -11 running reward: -8.48 epsilon: 0.5229550000039365\n",
            "Episode 2650 reward: -6 running reward: -8.39 epsilon: 0.5217535000039464\n",
            "Episode 2660 reward: -3 running reward: -8.11 epsilon: 0.5205745000039561\n",
            "Episode 2670 reward: -10 running reward: -8.24 epsilon: 0.5192425000039671\n",
            "Episode 2680 reward: -6 running reward: -8.47 epsilon: 0.5179870000039775\n",
            "Episode 2690 reward: -9 running reward: -8.65 epsilon: 0.5167360000039878\n",
            "Episode 2700 reward: -7 running reward: -8.1 epsilon: 0.5155705000039974\n",
            "Episode 2710 reward: -3 running reward: -7.71 epsilon: 0.5144185000040069\n",
            "Episode 2720 reward: -10 running reward: -7.9 epsilon: 0.5131270000040176\n",
            "Episode 2730 reward: -8 running reward: -7.18 epsilon: 0.5120290000040266\n",
            "Episode 2740 reward: -4 running reward: -7.15 epsilon: 0.5107375000040373\n",
            "Episode 2750 reward: -2 running reward: -7.24 epsilon: 0.5094955000040475\n",
            "Episode 2760 reward: -6 running reward: -7.5 epsilon: 0.5081995000040582\n",
            "Episode 2770 reward: -5 running reward: -7.38 epsilon: 0.5069215000040688\n",
            "Episode 2780 reward: -10 running reward: -7.23 epsilon: 0.5057335000040786\n",
            "Episode 2790 reward: 4 running reward: -7.26 epsilon: 0.504469000004089\n",
            "Episode 2800 reward: -4 running reward: -7.36 epsilon: 0.503258500004099\n",
            "Episode 2810 reward: -5 running reward: -7.62 epsilon: 0.5019895000041095\n",
            "Episode 2820 reward: -11 running reward: -7.28 epsilon: 0.5008510000041189\n",
            "Episode 2830 reward: -4 running reward: -7.35 epsilon: 0.49972150000412474\n",
            "Episode 2840 reward: -6 running reward: -6.78 epsilon: 0.4986865000041205\n",
            "Episode 2850 reward: -7 running reward: -6.6 epsilon: 0.4975255000041158\n",
            "Episode 2860 reward: -12 running reward: -6.1 epsilon: 0.4964545000041114\n",
            "Episode 2870 reward: -8 running reward: -6.09 epsilon: 0.4951810000041062\n",
            "Episode 2880 reward: -3 running reward: -5.92 epsilon: 0.49406950000410166\n",
            "Episode 2890 reward: -9 running reward: -5.68 epsilon: 0.49291300000409694\n",
            "Episode 2900 reward: -4 running reward: -5.61 epsilon: 0.4917340000040921\n",
            "Episode 2910 reward: -6 running reward: -5.38 epsilon: 0.49056850000408736\n",
            "Episode 2920 reward: -8 running reward: -5.55 epsilon: 0.4893535000040824\n",
            "Episode 2930 reward: -19 running reward: -5.92 epsilon: 0.4880575000040771\n",
            "Episode 2940 reward: -1 running reward: -6.4 epsilon: 0.486806500004072\n",
            "Episode 2950 reward: -7 running reward: -6.41 epsilon: 0.48564100000406724\n",
            "Episode 2960 reward: 2 running reward: -6.79 epsilon: 0.48439900000406216\n",
            "Episode 2970 reward: -9 running reward: -6.88 epsilon: 0.4830850000040568\n",
            "Episode 2980 reward: -8 running reward: -7.2 epsilon: 0.48182950000405167\n",
            "Episode 2990 reward: -5 running reward: -7.06 epsilon: 0.4807360000040472\n",
            "Episode 3000 reward: -17 running reward: -7.22 epsilon: 0.4794850000040421\n",
            "Episode 3010 reward: -11 running reward: -7.63 epsilon: 0.4781350000040366\n",
            "Episode 3020 reward: -12 running reward: -7.87 epsilon: 0.4768120000040312\n",
            "Episode 3030 reward: -7 running reward: -7.78 epsilon: 0.47555650000402605\n",
            "Episode 3040 reward: -1 running reward: -7.54 epsilon: 0.4744135000040214\n",
            "Episode 3050 reward: -2 running reward: -7.32 epsilon: 0.473347000004017\n",
            "Episode 3060 reward: -1 running reward: -7.08 epsilon: 0.4722130000040124\n",
            "Episode 3070 reward: -1 running reward: -6.66 epsilon: 0.4710880000040078\n",
            "Episode 3080 reward: -9 running reward: -6.6 epsilon: 0.4698595000040028\n",
            "Episode 3090 reward: -6 running reward: -6.78 epsilon: 0.468685000003998\n",
            "Episode 3100 reward: -2 running reward: -6.37 epsilon: 0.46761850000399363\n",
            "Episode 3110 reward: -9 running reward: -5.91 epsilon: 0.46647550000398896\n",
            "Episode 3120 reward: 0 running reward: -5.54 epsilon: 0.46531900000398424\n",
            "Episode 3130 reward: -2 running reward: -5.43 epsilon: 0.4641130000039793\n",
            "Episode 3140 reward: -2 running reward: -5.44 epsilon: 0.46296550000397463\n",
            "Episode 3150 reward: -4 running reward: -5.54 epsilon: 0.4618540000039701\n",
            "Episode 3160 reward: -12 running reward: -5.53 epsilon: 0.4607245000039655\n",
            "Episode 3170 reward: -3 running reward: -5.47 epsilon: 0.459626500003961\n",
            "Episode 3180 reward: -19 running reward: -5.4 epsilon: 0.4584295000039561\n",
            "Episode 3190 reward: 3 running reward: -4.88 epsilon: 0.45748900000395226\n",
            "Episode 3200 reward: -4 running reward: -4.94 epsilon: 0.4563955000039478\n",
            "Episode 3210 reward: -4 running reward: -4.9 epsilon: 0.4552705000039432\n",
            "Episode 3220 reward: -4 running reward: -4.71 epsilon: 0.4541995000039388\n",
            "Episode 3230 reward: -5 running reward: -4.56 epsilon: 0.4530610000039342\n",
            "Episode 3240 reward: -8 running reward: -4.62 epsilon: 0.4518865000039294\n",
            "Episode 3250 reward: -5 running reward: -4.98 epsilon: 0.4506130000039242\n",
            "Episode 3260 reward: -4 running reward: -4.9 epsilon: 0.4495195000039197\n",
            "Episode 3270 reward: -4 running reward: -4.84 epsilon: 0.44844850000391534\n",
            "Episode 3280 reward: -4 running reward: -4.57 epsilon: 0.44737300000391095\n",
            "Episode 3290 reward: -2 running reward: -4.98 epsilon: 0.44624800000390635\n",
            "Episode 3300 reward: -4 running reward: -4.88 epsilon: 0.44519950000390207\n",
            "Episode 3310 reward: -12 running reward: -4.95 epsilon: 0.44404300000389735\n",
            "Episode 3320 reward: -2 running reward: -5.02 epsilon: 0.44294050000389285\n",
            "Episode 3330 reward: -2 running reward: -4.93 epsilon: 0.44184250000388836\n",
            "Episode 3340 reward: -4 running reward: -4.95 epsilon: 0.4406590000038835\n",
            "Episode 3350 reward: -5 running reward: -4.51 epsilon: 0.43958350000387914\n",
            "Episode 3360 reward: -4 running reward: -4.49 epsilon: 0.4384990000038747\n",
            "Episode 3370 reward: -3 running reward: -4.45 epsilon: 0.4374460000038704\n",
            "Episode 3380 reward: -3 running reward: -4.56 epsilon: 0.4363210000038658\n",
            "Episode 3390 reward: -3 running reward: -4.65 epsilon: 0.43515550000386105\n",
            "Episode 3400 reward: -6 running reward: -4.72 epsilon: 0.43407550000385664\n",
            "Episode 3410 reward: -2 running reward: -4.35 epsilon: 0.4330855000038526\n",
            "Episode 3420 reward: -1 running reward: -4.2 epsilon: 0.43205050000384837\n",
            "Episode 3430 reward: -2 running reward: -4.09 epsilon: 0.4310020000038441\n",
            "Episode 3440 reward: -12 running reward: -3.67 epsilon: 0.43000750000384\n",
            "Episode 3450 reward: 8 running reward: -3.55 epsilon: 0.42898600000383585\n",
            "Episode 3460 reward: 2 running reward: -3.54 epsilon: 0.42790600000383144\n",
            "Episode 3470 reward: -7 running reward: -3.67 epsilon: 0.4267945000038269\n",
            "Episode 3480 reward: -6 running reward: -3.7 epsilon: 0.42565600000382225\n",
            "Episode 3490 reward: -10 running reward: -3.35 epsilon: 0.42464800000381814\n",
            "Episode 3500 reward: -1 running reward: -3.31 epsilon: 0.4235860000038138\n",
            "Episode 3510 reward: -2 running reward: -3.35 epsilon: 0.4225780000038097\n",
            "Episode 3520 reward: -3 running reward: -3.52 epsilon: 0.42146650000380514\n",
            "Episode 3530 reward: -9 running reward: -3.56 epsilon: 0.4204000000038008\n",
            "Episode 3540 reward: -12 running reward: -4.04 epsilon: 0.41918950000379585\n",
            "Episode 3550 reward: 1 running reward: -3.64 epsilon: 0.4183480000037924\n",
            "Episode 3560 reward: 1 running reward: -3.31 epsilon: 0.4174165000037886\n",
            "Episode 3570 reward: 0 running reward: -2.99 epsilon: 0.41644900000378465\n",
            "Episode 3580 reward: -3 running reward: -2.78 epsilon: 0.4154050000037804\n",
            "Episode 3590 reward: -1 running reward: -2.77 epsilon: 0.4144015000037763\n",
            "Episode 3600 reward: -1 running reward: -2.68 epsilon: 0.4133800000037721\n",
            "Episode 3610 reward: -6 running reward: -2.72 epsilon: 0.41235400000376793\n",
            "Episode 3620 reward: 0 running reward: -2.71 epsilon: 0.4112470000037634\n",
            "Episode 3630 reward: -5 running reward: -2.53 epsilon: 0.4102615000037594\n",
            "Episode 3640 reward: 3 running reward: -2.13 epsilon: 0.4092310000037552\n",
            "Episode 3650 reward: 2 running reward: -2.6 epsilon: 0.4081780000037509\n",
            "Episode 3660 reward: -4 running reward: -2.91 epsilon: 0.4071070000037465\n",
            "Episode 3670 reward: -3 running reward: -2.93 epsilon: 0.4061305000037425\n",
            "Episode 3680 reward: 3 running reward: -2.79 epsilon: 0.4051495000037385\n",
            "Episode 3690 reward: -11 running reward: -2.89 epsilon: 0.4041010000037342\n",
            "Episode 3700 reward: -5 running reward: -2.67 epsilon: 0.40317850000373046\n",
            "Episode 3710 reward: -10 running reward: -2.52 epsilon: 0.40222000000372654\n",
            "Episode 3720 reward: -4 running reward: -2.38 epsilon: 0.4011760000037223\n",
            "Episode 3730 reward: 1 running reward: -2.22 epsilon: 0.40026250000371855\n",
            "Episode 3740 reward: 1 running reward: -2.15 epsilon: 0.39926350000371447\n",
            "Episode 3750 reward: 4 running reward: -1.9 epsilon: 0.3983230000037106\n",
            "Episode 3760 reward: -3 running reward: -1.77 epsilon: 0.3973105000037065\n",
            "Episode 3770 reward: -7 running reward: -1.93 epsilon: 0.3962620000037022\n",
            "Episode 3780 reward: -4 running reward: -2.04 epsilon: 0.395231500003698\n",
            "Episode 3790 reward: 1 running reward: -1.85 epsilon: 0.39426850000369407\n",
            "Episode 3800 reward: -6 running reward: -2.08 epsilon: 0.3932425000036899\n",
            "Episode 3810 reward: 0 running reward: -2.21 epsilon: 0.3922255000036857\n",
            "Episode 3820 reward: 3 running reward: -2.46 epsilon: 0.391069000003681\n",
            "Episode 3830 reward: -2 running reward: -2.87 epsilon: 0.3899710000036765\n",
            "Episode 3840 reward: -9 running reward: -3.03 epsilon: 0.38890000000367214\n",
            "Episode 3850 reward: -5 running reward: -3.29 epsilon: 0.3878425000036678\n",
            "Episode 3860 reward: 5 running reward: -3.14 epsilon: 0.38689750000366396\n",
            "Episode 3870 reward: 3 running reward: -2.94 epsilon: 0.38593900000366005\n",
            "Episode 3880 reward: -5 running reward: -3.02 epsilon: 0.3848725000036557\n",
            "Episode 3890 reward: -3 running reward: -3.45 epsilon: 0.38371600000365097\n",
            "Episode 3900 reward: -4 running reward: -3.47 epsilon: 0.38268100000364674\n",
            "Episode 3910 reward: -7 running reward: -3.48 epsilon: 0.38165950000364257\n",
            "Episode 3920 reward: 1 running reward: -3.12 epsilon: 0.3806650000036385\n",
            "Episode 3930 reward: -6 running reward: -3.0 epsilon: 0.37962100000363425\n",
            "Episode 3940 reward: 3 running reward: -2.79 epsilon: 0.37864450000363026\n",
            "Episode 3950 reward: 5 running reward: -2.48 epsilon: 0.3777265000036265\n",
            "Episode 3960 reward: 2 running reward: -2.41 epsilon: 0.3768130000036228\n",
            "Episode 3970 reward: -11 running reward: -2.68 epsilon: 0.37573300000361837\n",
            "Episode 3980 reward: -5 running reward: -2.47 epsilon: 0.3747610000036144\n",
            "Episode 3990 reward: 6 running reward: -1.92 epsilon: 0.3738520000036107\n",
            "Episode 4000 reward: -5 running reward: -1.72 epsilon: 0.3729070000036068\n",
            "Episode 4010 reward: -3 running reward: -1.66 epsilon: 0.37191250000360276\n",
            "Episode 4020 reward: -6 running reward: -1.71 epsilon: 0.3708955000035986\n",
            "Episode 4030 reward: -7 running reward: -1.56 epsilon: 0.3699190000035946\n",
            "Episode 4040 reward: 2 running reward: -1.69 epsilon: 0.3688840000035904\n",
            "Episode 4050 reward: 5 running reward: -1.62 epsilon: 0.3679975000035868\n",
            "Episode 4060 reward: 1 running reward: -1.82 epsilon: 0.3669940000035827\n",
            "Episode 4070 reward: -5 running reward: -1.7 epsilon: 0.3659680000035785\n",
            "Episode 4080 reward: 2 running reward: -1.9 epsilon: 0.36490600000357415\n",
            "Episode 4090 reward: 0 running reward: -1.95 epsilon: 0.36397450000357034\n",
            "Episode 4100 reward: -10 running reward: -2.04 epsilon: 0.3629890000035663\n",
            "Episode 4110 reward: 0 running reward: -1.96 epsilon: 0.3620305000035624\n",
            "Episode 4120 reward: 0 running reward: -1.75 epsilon: 0.36110800000355864\n",
            "Episode 4130 reward: 4 running reward: -1.56 epsilon: 0.360217000003555\n",
            "Episode 4140 reward: -7 running reward: -1.5 epsilon: 0.3592090000035509\n",
            "Episode 4150 reward: 0 running reward: -1.69 epsilon: 0.3582370000035469\n",
            "Episode 4160 reward: -10 running reward: -1.91 epsilon: 0.3571345000035424\n",
            "Episode 4170 reward: 9 running reward: -1.59 epsilon: 0.3562525000035388\n",
            "Episode 4180 reward: -6 running reward: -1.5 epsilon: 0.35523100000353464\n",
            "Episode 4190 reward: -9 running reward: -1.99 epsilon: 0.35407900000352993\n",
            "Episode 4200 reward: -4 running reward: -2.0 epsilon: 0.3530890000035259\n",
            "Episode 4210 reward: -9 running reward: -1.9 epsilon: 0.35217550000352216\n",
            "Episode 4220 reward: -2 running reward: -2.06 epsilon: 0.3511810000035181\n",
            "Episode 4230 reward: 3 running reward: -2.06 epsilon: 0.35029000000351446\n",
            "Episode 4240 reward: 1 running reward: -1.85 epsilon: 0.3493765000035107\n",
            "Episode 4250 reward: -1 running reward: -1.71 epsilon: 0.348467500003507\n",
            "Episode 4260 reward: 1 running reward: -1.62 epsilon: 0.3474055000035027\n",
            "Episode 4270 reward: -2 running reward: -1.69 epsilon: 0.34649200000349895\n",
            "Episode 4280 reward: 0 running reward: -1.43 epsilon: 0.34558750000349525\n",
            "Episode 4290 reward: -2 running reward: -1.2 epsilon: 0.34453900000349097\n",
            "Episode 4300 reward: 0 running reward: -1.18 epsilon: 0.34355800000348696\n",
            "Episode 4310 reward: -2 running reward: -1.45 epsilon: 0.34252300000348274\n",
            "Episode 4320 reward: -7 running reward: -1.51 epsilon: 0.34150150000347856\n",
            "Episode 4330 reward: -2 running reward: -1.89 epsilon: 0.3404395000034742\n",
            "Episode 4340 reward: -11 running reward: -2.38 epsilon: 0.3393055000034696\n",
            "Episode 4350 reward: 0 running reward: -2.37 epsilon: 0.3384010000034659\n",
            "Episode 4360 reward: -4 running reward: -2.18 epsilon: 0.3374245000034619\n",
            "Episode 4370 reward: 3 running reward: -2.17 epsilon: 0.3365155000034582\n",
            "Episode 4380 reward: 2 running reward: -2.19 epsilon: 0.33560200000345447\n",
            "Episode 4390 reward: 6 running reward: -1.77 epsilon: 0.33474250000345096\n",
            "Episode 4400 reward: 2 running reward: -1.7 epsilon: 0.3337930000034471\n",
            "Episode 4410 reward: -1 running reward: -1.32 epsilon: 0.33292900000344355\n",
            "Episode 4420 reward: -13 running reward: -1.49 epsilon: 0.33183100000343907\n",
            "Episode 4430 reward: -2 running reward: -1.22 epsilon: 0.33089050000343523\n",
            "Episode 4440 reward: 7 running reward: -0.69 epsilon: 0.32999500000343157\n",
            "Episode 4450 reward: -7 running reward: -0.8 epsilon: 0.3290410000034277\n",
            "Episode 4460 reward: -1 running reward: -0.41 epsilon: 0.3282400000034244\n",
            "Episode 4470 reward: 1 running reward: -0.38 epsilon: 0.32734450000342075\n",
            "Episode 4480 reward: -7 running reward: -0.34 epsilon: 0.3264490000034171\n",
            "Episode 4490 reward: -4 running reward: -0.45 epsilon: 0.3255400000034134\n",
            "Episode 4500 reward: -2 running reward: -0.26 epsilon: 0.32467600000340985\n",
            "Episode 4510 reward: 0 running reward: -0.39 epsilon: 0.3237535000034061\n",
            "Episode 4520 reward: -2 running reward: 0.01 epsilon: 0.32283550000340233\n",
            "Episode 4530 reward: 0 running reward: 0.06 epsilon: 0.3219175000033986\n",
            "Episode 4540 reward: 0 running reward: 0.15 epsilon: 0.3210625000033951\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[35], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_steps_per_episode):\n\u001b[0;32m      8\u001b[0m   frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m   action \u001b[38;5;241m=\u001b[39m get_greedy_epsilon(model, state, action_mask)\n\u001b[0;32m     12\u001b[0m   state_next, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep((action \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m, action \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m     13\u001b[0m   state_next \u001b[38;5;241m=\u001b[39m preprocess_state(state_next)\n",
            "Cell \u001b[1;32mIn[31], line 12\u001b[0m, in \u001b[0;36mget_greedy_epsilon\u001b[1;34m(model, state, mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# compute the q-values\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m q_values \u001b[38;5;241m=\u001b[39m model(state_tensor)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# select the q-values of valid actions\u001b[39;00m\n\u001b[0;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(\n\u001b[0;32m     15\u001b[0m   q_values\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100.\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[19], line 15\u001b[0m, in \u001b[0;36mQModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for _ in range(max_episodes):\n",
        "  state, info = env.reset()\n",
        "  state = preprocess_state(state)\n",
        "  action_mask = info['action_mask'].reshape((-1,))\n",
        "  episode_reward = 0\n",
        "\n",
        "  for timestep in range(1, max_steps_per_episode):\n",
        "    frame_count += 1\n",
        "\n",
        "    action = get_greedy_epsilon(model, state, action_mask)\n",
        "\n",
        "    state_next, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "    state_next = preprocess_state(state_next)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "\n",
        "    episode_reward += reward\n",
        "\n",
        "    action_history.append(action)\n",
        "    action_mask_history.append(action_mask)\n",
        "    state_history.append(state)\n",
        "    state_next_history.append(state_next)\n",
        "    rewards_history.append(reward)\n",
        "    done_history.append(done)\n",
        "\n",
        "    state = state_next\n",
        "\n",
        "    if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "      update_network()\n",
        "\n",
        "    if frame_count % update_target_network == 0:\n",
        "      model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "    if len(rewards_history) > max_memory_length:\n",
        "      del rewards_history[:1]\n",
        "      del state_history[:1]\n",
        "      del state_next_history[:1]\n",
        "      del action_history[:1]\n",
        "      del action_mask_history[:1]\n",
        "      del done_history[:1]\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  episode_count +=1\n",
        "  episode_reward_history.append(episode_reward)\n",
        "\n",
        "  if len(episode_reward_history)> 100:\n",
        "    del episode_reward_history[0]\n",
        "\n",
        "  running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "  if episode_count % 10 == 0:\n",
        "    print(f'Episode {episode_count} reward: {episode_reward} running reward: {running_reward} epsilon: {epsilon}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5_QSgHxCdQe",
        "outputId": "3ac268fb-5aa0-4d80-dd03-a42bf33c513d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    MH   |      H  \n",
            "   H H   |    H H  \n",
            "  MHHHH  |    HHHH \n",
            "   HMH   |    H H  \n",
            "   MMH   |      H  \n",
            "     MM  |         \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "while not done:\n",
        "  action = get_greedy_action(model, state, action_mask)\n",
        "  print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "  sys.stdout.flush()\n",
        "  time.sleep(1.0)\n",
        "  clear_output(wait=False)\n",
        "  board, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "  state = preprocess_state(board)\n",
        "  action_mask = info['action_mask'].reshape((-1,))\n",
        "  env.render()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
