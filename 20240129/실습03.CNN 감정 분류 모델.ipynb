{"cells":[{"cell_type":"code","execution_count":286,"metadata":{"id":"_uXOElQYPQ6-"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":287,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Q</th>\n","      <th>A</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12시 땡!</td>\n","      <td>하루가 또 가네요.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1지망 학교 떨어졌어</td>\n","      <td>위로해 드립니다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3박4일 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3박4일 정도 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PPL 심하네</td>\n","      <td>눈살이 찌푸려지죠.</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Q            A  label\n","0           12시 땡!   하루가 또 가네요.      0\n","1      1지망 학교 떨어졌어    위로해 드립니다.      0\n","2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n","3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n","4          PPL 심하네   눈살이 찌푸려지죠.      0"]},"execution_count":287,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('chatbot_data.csv')\n","df.head()"]},{"cell_type":"code","execution_count":288,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["label\n","0    5290\n","1    3570\n","2    2963\n","Name: count, dtype: int64\n"]}],"source":["print(df['label'].value_counts())"]},{"cell_type":"code","execution_count":289,"metadata":{},"outputs":[],"source":["features = df['Q'].to_list()\n","labels = df['label'].to_list()"]},{"cell_type":"code","execution_count":290,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","import nltk\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":291,"metadata":{},"outputs":[],"source":["corpus =  [word_tokenize(x) for x in features]\n","# Convert the corpus to word index sequences\n","word_index = {}\n","corpus_indices = []\n","for text in features:\n","    indices = [word_index.setdefault(word, len(word_index)) for word in word_tokenize(text)]\n","    corpus_indices.append(torch.tensor(indices))\n","\n","# Pad the sequences to a fixed length\n","MAX_SEQ_LEN = 15\n","padded_seqs = pad_sequence(corpus_indices, batch_first=True, padding_value=0)\n"]},{"cell_type":"code","execution_count":292,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['12시', '땡', '!']\n","tensor([0, 1, 2])\n","tensor([0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"]}],"source":["print(corpus[0])\n","print(corpus_indices[0])\n","print(padded_seqs[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":293,"metadata":{},"outputs":[],"source":["class ChatModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_classes, dropout):\n","        super(ChatModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.conv1 = nn.Conv1d(embed_size, 128, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n","        self.maxpool = nn.MaxPool1d(kernel_size=2)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.permute(0, 2, 1)\n","        x = F.relu(self.conv1(x))\n","        x = self.maxpool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.maxpool(x)\n","        x = x.flatten(start_dim=1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":294,"metadata":{},"outputs":[],"source":["class ChatDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","        \n","    def __len__(self):\n","        return len(self.X)\n","    \n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]"]},{"cell_type":"code","execution_count":295,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = ChatModel(len(word_index), 128, 3, 0.2)\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","\n","EPOCHS = 10\n","BATCH_SIZE = 32"]},{"cell_type":"code","execution_count":296,"metadata":{},"outputs":[],"source":["dataset = ChatDataset(padded_seqs, torch.tensor(labels))\n","train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"code","execution_count":297,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 01, loss: 0.973\n","Epoch: 02, loss: 0.506\n","Epoch: 03, loss: 0.441\n","Epoch: 04, loss: 0.443\n","Epoch: 05, loss: 0.024\n","Epoch: 06, loss: 0.006\n","Epoch: 07, loss: 0.119\n","Epoch: 08, loss: 0.013\n","Epoch: 09, loss: 0.011\n","Epoch: 10, loss: 0.017\n"]}],"source":["for epoch in range(EPOCHS):\n","    for batch_x, batch_y in train_loader:\n","        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n","        \n","        output = model(batch_x)\n","        loss = criterion(output, batch_y)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    print(f'Epoch: {epoch+1:02}, loss: {loss.item():.3f}')"]},{"cell_type":"code","execution_count":298,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[3885, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Predicted class: 0\n"]}],"source":["text = \"안녕\"\n","text = word_tokenize(text)\n","text = [word_index.get(word, 0) for word in text]  # 0 for unknown words\n","text += [0] * (MAX_SEQ_LEN - len(text) + 1)\n","print(text)\n","\n","text_tensor = torch.tensor(text).unsqueeze(0)  # Add batch dimension\n","text_tensor = text_tensor.to(device)\n","\n","model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():  # Disable gradient calculation\n","    output = model(text_tensor)\n","\n","probabilities = torch.softmax(output, dim=1)\n","predicted_class = torch.argmax(probabilities, dim=1)\n","\n","print(\"Predicted class:\", predicted_class.item())\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyObtMBrton56eYJCOqHuSfS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
